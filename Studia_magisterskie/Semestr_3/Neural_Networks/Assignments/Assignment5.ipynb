{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faca5110",
   "metadata": {},
   "source": [
    "# Assigment 5\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* last lab before 27.06.2022 \n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Consider the vowel reconstruction task -- i.e. inserting missing vowels (aeuioy) to obtain proper English text. For instance for the input sentence:\n",
    "\n",
    "<pre>\n",
    "h m gd smbd hs stln ll m vwls\n",
    "</pre>\n",
    "\n",
    "the best result is\n",
    "\n",
    "<pre>\n",
    "oh my god somebody has stolen all my vowels\n",
    "</pre>\n",
    "\n",
    "In this task both dev and test data come from the two books about Winnie-the-Pooh. You have to train two RNN Language Models on *pooh-train.txt*. For the first model use the code below, for the second choose different hyperparameters (different dropout, smaller number of units or layers, or just do any modification you want). \n",
    "\n",
    "The code below is based on\n",
    "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6bafae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import gensim.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from collections import defaultdict as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d50e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "SEQUENCE_LENGTH = 15\n",
    "\n",
    "class PoohDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_length, device, data_file = './data/pooh_train.txt', words_file = \"./data/pooh_words.txt\"):\n",
    "        txt = open(data_file).read()\n",
    "        \n",
    "        self.words = utils.simple_preprocess(txt, min_len=1)\n",
    "\n",
    "        self.uniq_words = set(utils.simple_preprocess(open(words_file).read(), min_len=1))\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
    "        )\n",
    "        \n",
    "pooh_dataset = PoohDataset(SEQUENCE_LENGTH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b22a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, dataset, device, lstm_size = 512, embedding_dim = 100, num_layers = 2, dropout = 0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
    "        \n",
    "# model = LSTMModel(pooh_dataset, device) \n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074d5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, batch_size = 512, max_epochs = 30):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
    "        \n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "            \n",
    "# train(pooh_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92ad0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/pooh_2x512_30ep_v1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/pooh_2x512_30ep.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38520dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the code if you want\n",
    "\n",
    "def devowelize(s):\n",
    "    vowels = set(\"aoiuye'\")\n",
    "    rv = ''.join(a for a in s if a not in vowels)\n",
    "    if rv:\n",
    "        return rv\n",
    "    return '_' # Symbol for words without consonants\n",
    "\n",
    "pooh_words = set(utils.simple_preprocess(open('./data/pooh_words.txt').read()))\n",
    "representation = dd(set)\n",
    "\n",
    "for w in pooh_words:\n",
    "    r = devowelize(w)\n",
    "    representation[r].add(w)\n",
    "    \n",
    "hard_words = set()\n",
    "for r, ws in representation.items():\n",
    "    if len(ws) > 1:\n",
    "        hard_words.update(ws)\n",
    "        \n",
    "def reconstruction(sentence):\n",
    "    result = [[]]\n",
    "    for word in sentence:\n",
    "        variants = representation[word]\n",
    "        result = [prefix + [v] for v in variants for prefix in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict(dataset, model, text, repeats = 1):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        if type(text) == str:\n",
    "            words = text.split()\n",
    "        else:\n",
    "            words = text\n",
    "\n",
    "            \n",
    "        state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "        variants = reconstruction(words)\n",
    "\n",
    "        batch_size = 1000\n",
    "\n",
    "        variants_log_probs_sum = torch.zeros(len(variants))\n",
    "\n",
    "        for i in range(0, len(variants), batch_size):\n",
    "            x = torch.tensor([[dataset.word_to_index[w] for w in ws] for ws in variants[i:i+batch_size]], dtype=torch.long)\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred, _ = model(x, (state_h, state_c))\n",
    "\n",
    "            log_probs = nn.functional.log_softmax(y_pred, dim=2)[:,:-1,:]\n",
    "\n",
    "            probs_indices = x[:, 1:]\n",
    "            correct_words_log_probs = torch.tensor([[log_probs[i,j, probs_indices[i,j]].item() for j in range(probs_indices.shape[1])]\n",
    "                for i in range(probs_indices.shape[0])])\n",
    "\n",
    "            variants_log_probs_sum[i:i+batch_size] = correct_words_log_probs.sum(dim=1)\n",
    "\n",
    "        # variants_log_probs_sum = correct_words_log_probs.sum(dim=1)\n",
    "        variants_probs = nn.functional.softmax(variants_log_probs_sum, dim=0).numpy()\n",
    "\n",
    "        variant_picks = np.zeros(len(variants))\n",
    "\n",
    "        for _ in range(repeats):\n",
    "            index = np.random.choice(len(variants), p=variants_probs)\n",
    "            variant_picks[index] += 1\n",
    "\n",
    "        best_index = np.argmax(variant_picks)\n",
    "\n",
    "        return ' '.join(variants[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273881ae",
   "metadata": {},
   "source": [
    "You can assume that only words from pooh_words.txt can occur in the reconstructed text. For decoding you have two options (choose one, or implement both ang get **+1** bonus point)\n",
    "\n",
    "1. Sample reconstructed text several times (with quite a low temperature), choose the most likely result.\n",
    "2. Perform beam search.\n",
    "\n",
    "Of course in the sampling procedure you should consider only words matching the given consonants.\n",
    "\n",
    "Report accuracy of your methods (for both language models). The accuracy should be computed by the following function, it should be *greater than 0.25*.\n",
    "\n",
    "\n",
    "```python\n",
    "def accuracy(original_sequence, reconstructed_sequence):\n",
    "    sa = original_sequence\n",
    "    sb = reconstructed_sequence\n",
    "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
    "    return score / len(original_sequence)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d79ee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(original_sequence, reconstructed_sequence):\n",
    "    sa = original_sequence\n",
    "    sb = reconstructed_sequence\n",
    "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
    "    # print(score)\n",
    "    return score / len(original_sequence)\n",
    "\n",
    "def test_sequence(dataset, model, seq):\n",
    "    devowelized = list(map(devowelize, seq))\n",
    "\n",
    "    reconstructed = predict(dataset, model, devowelized, repeats=30)\n",
    "    \n",
    "    # print(f\"{' '.join(seq)} | {reconstructed}\")\n",
    "\n",
    "    return accuracy(seq, reconstructed.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7da6817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'missage', 'he', 'said', 'to', 'himself', 'that', 'what', 'it', 'is', 'and', 'that', 'letter', 'is', 'and', 'so', 'is', 'that', 'and', 'so']\n"
     ]
    }
   ],
   "source": [
    "test_file = open(\"data/pooh_test.txt\")\n",
    "test_data = utils.simple_preprocess(test_file.read())\n",
    "print(test_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "491b4c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model score: 0.53\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sample_data(data, n_samples, seq_length):\n",
    "    indices = np.random.choice(len(data)-seq_length, n_samples)\n",
    "\n",
    "    return [data[i:i+seq_length] for i in indices]\n",
    "\n",
    "seq_len = 5\n",
    "data = sample_data(test_data, 100, seq_len)\n",
    "\n",
    "score = np.mean([test_sequence(pooh_dataset, model, seq) for seq in data])\n",
    "\n",
    "print(f\"Base model score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4df76cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamil/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(2581, 50)\n",
       "  (lstm): LSTM(50, 256, dropout=0.2)\n",
       "  (fc): Linear(in_features=256, out_features=2581, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = LSTMModel(pooh_dataset, device, lstm_size=256, embedding_dim=50, num_layers=1) \n",
    "model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b828aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 89, 'loss': 6.095155239105225}\n",
      "{'epoch': 1, 'batch': 89, 'loss': 5.66037654876709}\n",
      "{'epoch': 2, 'batch': 89, 'loss': 5.257487773895264}\n",
      "{'epoch': 3, 'batch': 89, 'loss': 4.873077392578125}\n",
      "{'epoch': 4, 'batch': 89, 'loss': 4.4854044914245605}\n",
      "{'epoch': 5, 'batch': 89, 'loss': 4.171422958374023}\n",
      "{'epoch': 6, 'batch': 89, 'loss': 3.836507797241211}\n",
      "{'epoch': 7, 'batch': 89, 'loss': 3.4898197650909424}\n",
      "{'epoch': 8, 'batch': 89, 'loss': 3.2256109714508057}\n",
      "{'epoch': 9, 'batch': 89, 'loss': 2.9390370845794678}\n",
      "{'epoch': 10, 'batch': 89, 'loss': 2.67284893989563}\n",
      "{'epoch': 11, 'batch': 89, 'loss': 2.414641857147217}\n",
      "{'epoch': 12, 'batch': 89, 'loss': 2.208237409591675}\n",
      "{'epoch': 13, 'batch': 89, 'loss': 1.9928768873214722}\n",
      "{'epoch': 14, 'batch': 89, 'loss': 1.7707672119140625}\n",
      "{'epoch': 15, 'batch': 89, 'loss': 1.5765235424041748}\n",
      "{'epoch': 16, 'batch': 89, 'loss': 1.399997591972351}\n",
      "{'epoch': 17, 'batch': 89, 'loss': 1.2500643730163574}\n",
      "{'epoch': 18, 'batch': 89, 'loss': 1.1683738231658936}\n",
      "{'epoch': 19, 'batch': 89, 'loss': 1.0494036674499512}\n",
      "{'epoch': 20, 'batch': 89, 'loss': 0.9498642086982727}\n",
      "{'epoch': 21, 'batch': 89, 'loss': 0.8595749735832214}\n",
      "{'epoch': 22, 'batch': 89, 'loss': 0.8151829838752747}\n",
      "{'epoch': 23, 'batch': 89, 'loss': 0.7425432205200195}\n",
      "{'epoch': 24, 'batch': 89, 'loss': 0.6677172183990479}\n",
      "{'epoch': 25, 'batch': 89, 'loss': 0.5935436487197876}\n",
      "{'epoch': 26, 'batch': 89, 'loss': 0.5249011516571045}\n",
      "{'epoch': 27, 'batch': 89, 'loss': 0.4947647154331207}\n",
      "{'epoch': 28, 'batch': 89, 'loss': 0.44764137268066406}\n",
      "{'epoch': 29, 'batch': 89, 'loss': 0.41653090715408325}\n"
     ]
    }
   ],
   "source": [
    "train(pooh_dataset, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77d98d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd model score: 0.53\n"
     ]
    }
   ],
   "source": [
    "score = np.mean([test_sequence(pooh_dataset, model_2, seq) for seq in data])\n",
    "\n",
    "print(f\"2nd model score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a158dfd",
   "metadata": {},
   "source": [
    "## Task 2 (6 points)\n",
    "\n",
    "This task is about text generation. You have to:\n",
    "\n",
    "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
    "\n",
    "**B**. choose the tokenization procedure. It should have two stages:\n",
    "\n",
    "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
    "\n",
    "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest suffix of x in S\n",
    "\n",
    "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
    "\n",
    "1. it should use the RNN language model (trained on sub-word tokens)\n",
    "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
    "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
    "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X) \n",
    "5. in generated texts every token 3-gram should be uniq\n",
    "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613b18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nie dokończyłem 2 zadania, BooksDataset dzieli na tokeny w sposób opisany w B2\n",
    "\n",
    "import nltk\n",
    "\n",
    "W_SIZE = 8000\n",
    "PS_SIZE = 1000\n",
    "\n",
    "\n",
    "class BooksDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequence_length, device, file_paths):\n",
    "        txt = \"\\n\\n\\n\\n\".join([open(f).read() for f in file_paths])\n",
    "        \n",
    "        self.words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "        self.unique_words = self.get_unique_words()\n",
    "\n",
    "        self.tokens = self.subword_tokenization()\n",
    "\n",
    "        self.unique_tokens = self.get_unique_tokens()\n",
    "\n",
    "        self.uniq_words = self.unique_tokens\n",
    "\n",
    "        self.index_to_token = {index: token for index, token in enumerate(self.unique_tokens)}\n",
    "        self.token_to_index = {token: index for index, token in enumerate(self.unique_tokens)}\n",
    "\n",
    "        self.tokens_indexes = [self.token_to_index[t] for t in self.tokens]\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def subword_tokenization(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        self.word_tokens = set(sorted(word_counts, key=word_counts.get, reverse=True)[:W_SIZE])\n",
    "        self.prefix_tokens = self.get_prefix_tokens()\n",
    "        self.suffix_tokens = self.get_suffix_tokens()\n",
    "        self.unique_tokens = self.word_tokens | self.prefix_tokens | self.suffix_tokens\n",
    "\n",
    "        return [t for w in self.words for t in self.tokenize_word(w)]\n",
    "\n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        if word in self.word_tokens:\n",
    "            return [word, \" \"]\n",
    "\n",
    "        wl = len(word)\n",
    "\n",
    "        possible_tokenizes = [[word[:i], word[i:j], word[j:]] for i in range(wl) for j in range(i, wl)\n",
    "            if word[:i] in self.prefix_tokens and word[j:] in self.suffix_tokens]\n",
    "\n",
    "\n",
    "        if len(possible_tokenizes) == 0:\n",
    "            return [word, \" \"]\n",
    "\n",
    "        index = np.argmax([len(t[0])**2 + len(t[2])**2 for t in possible_tokenizes])\n",
    "\n",
    "        return possible_tokenizes[index] + [\" \"]\n",
    "        \n",
    "        \n",
    "    def get_most_popular(self, count):\n",
    "        return set(sorted(count, key= lambda key: (count.get(key), len(key)), reverse=True)[:PS_SIZE])\n",
    "        \n",
    "\n",
    "    def get_prefix_tokens(self):\n",
    "        prefix_count = dd(int)\n",
    "\n",
    "        for word in self.words:\n",
    "            for i in range(1, len(word)):\n",
    "                prefix_count[word[:i]] += 1\n",
    "\n",
    "        result = self.get_most_popular(prefix_count)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_suffix_tokens(self):\n",
    "        suffix_count = dd(int)\n",
    "\n",
    "        for word in self.words:\n",
    "            for i in range(0, len(word)-1):\n",
    "                suffix_count[word[i:]] += 1\n",
    "\n",
    "        result = self.get_most_popular(suffix_count)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_unique_tokens(self):\n",
    "        token_counts = Counter(self.tokens)\n",
    "        return sorted(token_counts, key=token_counts.get, reverse=True)\n",
    "\n",
    "    def get_unique_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.tokens_indexes[index:index+self.sequence_length], device=self.device),\n",
    "            torch.tensor(self.tokens_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/books/Heretics.txt', './data/books/Orthodoxy.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# book_paths = [\"./data/books/\" + title for title in os.listdir(\"./data/books/\")][:5]\n",
    "book_paths = [\"./data/books/Heretics.txt\", \"./data/books/Orthodoxy.txt\"]\n",
    "\n",
    "print(book_paths)\n",
    "\n",
    "books_dataset = BooksDataset(SEQUENCE_LENGTH, device, book_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4de3aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(14422, 100)\n",
       "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=14422, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(books_dataset, device) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(books_dataset, model, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/books.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23067f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, words, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e84582",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "In this task you have to create a network which looks at characters of the word and tries to guess whether the word is a noun, a verb, an adjective, and so on. To be more precise: the input is a word (without context), the output is a POS-tag (Part-of-Speech). Since some words are unambiguous, and we have no context, our network is supposed to return the set of possible tags.\n",
    "\n",
    "The data is taken from Universal Dependencies English corpus, and of course it contains errors, especially because not all possible tags occured in the data.\n",
    "\n",
    "Train a network (4p) or two networks (+2p) solving this task. Both networks should look at character n-grams occuring in the word. There are two options:\n",
    "\n",
    "* **Fixed size:** for instance take 2,3, and 4-character suffixes of the word, use them as  features (whith 1-hot encoding). You can also combine prefix and suffix features. Simple, useful trick: when looking at suffixes, add some '_' characters at the beginning of the word to guarantee that shorter words have suffixes of a desired length.\n",
    "\n",
    "* **Variable size:** take for instance 4-grams (or 4 grams and 3-grams), use Deep Averaging Network. Simple trick: add extra character at the beginning and at the end of the word, to add the information, that ngram occurs at special position ('ed' at the end has slightly different meaning that 'ed' in the middle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac85fcb",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Apply seq2seq model (you can modify the code from this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to compute grapheme to phoneme conversion for English. Train the model on dev_cmu_dict.txt and test it on test_cmu_dict.txt. Report accuracy of your solution using two metrics:\n",
    "* exact match (how many words are perfectly converted to phonemes)\n",
    "* exact match without stress (how many words are perfectly converted to phonemes when we remove the information about stress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da193ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d5e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538fb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4feefe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
