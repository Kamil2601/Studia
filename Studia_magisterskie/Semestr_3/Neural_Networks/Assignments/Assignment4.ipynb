{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef34449",
   "metadata": {},
   "source": [
    "### Assigment 4\n",
    "\n",
    "**Submission deadlines**:\n",
    "\n",
    "* get at least 4 points by Tuesday, 12.05.2022\n",
    "* remaining points: last lab session before or on Tuesday, 19.05.2022\n",
    "\n",
    "**Points:** Aim to get 12 out of 15+ possible points\n",
    "\n",
    "All needed data files are on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> (or will be soon :) )\n",
    "\n",
    "## Task 1 (5 points)\n",
    "\n",
    "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
    "\n",
    "<pre>\n",
    "word1 x1_1 x1_2 ... x1_N \n",
    "word2 x2_1 x2_2 ... x2_N\n",
    "...\n",
    "wordK xK_1 xK_2 ... xk_N\n",
    "</pre>\n",
    "\n",
    "Use the loss from Slide 3 in Lecture NLP.2, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
    "\n",
    "**Remark**: the data is specially prepared to make the learning process easier. \n",
    "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f801c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from numba import njit\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "284bb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nagromadzenie' 'G2_następstwo']\n",
      " ['temat' 'G2_skarbnica']\n",
      " ['zaspokojenie' 'G1_pragnienie']\n",
      " ['dudkiewicz' 'SUBJ_pokonać']\n",
      " ['odpis' 'AND_wyciąg']\n",
      " ['entuzjazm' 'AND_znajomość']\n",
      " ['zakład' 'G1_alpinizm']\n",
      " ['ręka' 'przeciwny']\n",
      " ['odroczenie' 'G1_realizacja']\n",
      " ['rysunek' 'AND_górnik']]\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"./data/task1_objects_contexts_polish.txt\")\n",
    "\n",
    "data_lines = data_file.readlines()\n",
    "# data_lines = [re.sub(\"[ ].+_\", \" \", line) for line in data_file.readlines()]\n",
    "\n",
    "full_data = [line.rstrip().split(\" \") for line in data_lines]\n",
    "full_data = np.array(full_data)\n",
    "\n",
    "print(full_data[:10])\n",
    "\n",
    "# data = data[:5000]\n",
    "# print(data.shape)\n",
    "\n",
    "# print(np.unique(data).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce3c9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184246\n"
     ]
    }
   ],
   "source": [
    "data = full_data\n",
    "unique_words = np.unique(data)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0d4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(unique_words)}\n",
    "index_to_word = unique_words\n",
    "\n",
    "\n",
    "def word_to_one_hot_vector(word):\n",
    "    result = np.zeros(len(word_to_index))\n",
    "    result[word_to_index[word]] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c026f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    def __init__(self, words):\n",
    "        sorted_words = np.sort(words)\n",
    "        self.words, counts = np.unique(sorted_words, return_counts=True)\n",
    "        self.indices = np.arange(len(self.words))\n",
    "        self.probabilites = counts / counts.sum()\n",
    "        self.probabilites = self.probabilites ** 0.75 / (self.probabilites ** 0.75).sum()\n",
    "\n",
    "    def get(self, size=None):\n",
    "        return np.random.choice(a = self.words, size=size, p = self.probabilites)\n",
    "\n",
    "    def get_index(self, size=None):\n",
    "        return np.random.choice(a = self.indices, size=size, p = self.probabilites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3c67753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5b853a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, vec_size, vocabulary_size):\n",
    "        self.vec_size = vec_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        self.W_word = np.random.rand(vocabulary_size, vec_size) * 10 - 5\n",
    "        self.W_context = np.random.rand(vocabulary_size, vec_size) * 10 - 5\n",
    "\n",
    "        self.W_word = torch.from_numpy(self.W_word).float().to(device=\"cuda\")\n",
    "        self.W_context = torch.from_numpy(self.W_context).float().to(device=\"cuda\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = x @ self.W_word\n",
    "\n",
    "        z = v @ self.W_context.T\n",
    "\n",
    "        return v, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054ac56",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial v_x} \\log{\\sigma(v_c \\cdot v_x)}\n",
    "&= \\frac{1}{\\sigma(v_c \\cdot v_x)} \\cdot \\frac{\\partial}{\\partial v_x} \\sigma(v_c \\cdot v_x) \\\\\n",
    "&= \\frac{1}{\\sigma(v_c \\cdot v_x)} \\cdot \\sigma(v_c \\cdot v_x)(1-\\sigma(v_c \\cdot v_x)) \\cdot \\frac{\\partial}{\\partial v_x} (v_c \\cdot v_x) \\\\\n",
    "&= (1-\\sigma(v_c \\cdot v_x)) \\cdot v_c\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c0e8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sigmoid_derivative(vc, vx):    \n",
    "    return (1 - sigmoid(torch.dot(vc, vx))) * vc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c11363d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17699/5525116 [01:00<5:16:14, 290.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kamil/University/Studia_magisterskie/Semestr_3/Neural_Networks/Assignments/Assignment4 copy.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kamil/University/Studia_magisterskie/Semestr_3/Neural_Networks/Assignments/Assignment4%20copy.ipynb#ch0000010?line=22'>23</a>\u001b[0m W_word_grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(net\u001b[39m.\u001b[39mW_word)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kamil/University/Studia_magisterskie/Semestr_3/Neural_Networks/Assignments/Assignment4%20copy.ipynb#ch0000010?line=23'>24</a>\u001b[0m W_context_grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(net\u001b[39m.\u001b[39mW_context)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kamil/University/Studia_magisterskie/Semestr_3/Neural_Networks/Assignments/Assignment4%20copy.ipynb#ch0000010?line=25'>26</a>\u001b[0m W_word_grad[word_index] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlog_sigmoid_derivative(u_c, v_w) \u001b[39m-\u001b[39m \u001b[39msum\u001b[39m([log_sigmoid_derivative(\u001b[39m-\u001b[39mur, v_w) \u001b[39mfor\u001b[39;00m ur \u001b[39min\u001b[39;00m u_rs])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kamil/University/Studia_magisterskie/Semestr_3/Neural_Networks/Assignments/Assignment4%20copy.ipynb#ch0000010?line=26'>27</a>\u001b[0m W_context_grad[context_index] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlog_sigmoid_derivative(v_w, u_c)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kamil/University/Studia_magisterskie/Semestr_3/Neural_Networks/Assignments/Assignment4%20copy.ipynb#ch0000010?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m rci, u_r \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(random_contexts_index, u_rs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vec_size = 50\n",
    "k = 5\n",
    "learning_rate = 0.003\n",
    "num_epochs = 5\n",
    "\n",
    "sampler = NegativeSampler(data[:,1])\n",
    "net = Network(vec_size=vec_size, vocabulary_size=len(unique_words))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in tqdm(range(len(data))):\n",
    "        word, context = data[i]\n",
    "        word_index = word_to_index[word]\n",
    "        context_index = word_to_index[word]\n",
    "\n",
    "        u_c = net.W_context[context_index]\n",
    "        v_w = net.W_word[word_index]\n",
    "\n",
    "        random_contexts_index = sampler.get_index(k)\n",
    "        \n",
    "        u_rs = net.W_context[random_contexts_index]\n",
    "\n",
    "        W_word_grad = torch.zeros_like(net.W_word)\n",
    "        W_context_grad = torch.zeros_like(net.W_context)\n",
    "\n",
    "        W_word_grad[word_index] += -log_sigmoid_derivative(u_c, v_w) - sum([log_sigmoid_derivative(-ur, v_w) for ur in u_rs])\n",
    "        W_context_grad[context_index] += -log_sigmoid_derivative(v_w, u_c)\n",
    "\n",
    "        for rci, u_r in zip(random_contexts_index, u_rs):\n",
    "            W_context_grad[rci] -= log_sigmoid_derivative(-v_w, u_r)\n",
    "\n",
    "        net.W_word -= learning_rate * W_word_grad\n",
    "        net.W_context -= learning_rate * W_context_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b023789",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('task1_w2v_vectors.txt', \"w\")\n",
    "\n",
    "word_count = 0\n",
    "\n",
    "for i, word in enumerate(unique_words):\n",
    "    if re.search(\"[_]\", word) == None:\n",
    "        word_count += 1\n",
    "\n",
    "\n",
    "f.write(str(word_count) + \" \" + str(vec_size) + \"\\n\")\n",
    "\n",
    "for i, word in enumerate(unique_words):\n",
    "    if re.search(\"[_]\", word) == None:\n",
    "        # print(i, word, net.W_word[i])\n",
    "        # print(np.array2string(net.W_word[i] , precision=5, separator=\" \")[1:-1])\n",
    "        line = word + \" \" + \" \".join([str(n)[:7] for n in net.W_word[i]]) + \"\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae909d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pies\n",
      "    niepodważalny 0.6319450736045837\n",
      "    underground 0.545934796333313\n",
      "    półfinalistka 0.5200155377388\n",
      "    madrygał 0.5080426931381226\n",
      "    popularność 0.4967035949230194\n",
      "    jarzmo 0.4937137961387634\n",
      "    pięciokrotny 0.4878771901130676\n",
      "    nadtlenek 0.4850349724292755\n",
      "    rozsypywanie 0.46927425265312195\n",
      "    genetyzm 0.46138328313827515\n",
      "\n",
      "WORD: smok\n",
      "    rod 0.607578694820404\n",
      "    znieważanie 0.5199453830718994\n",
      "    szofer 0.4913886785507202\n",
      "    ślub 0.47276371717453003\n",
      "    gildia 0.4708929657936096\n",
      "    dociskanie 0.4677221179008484\n",
      "    zły 0.46679234504699707\n",
      "    barak 0.45267611742019653\n",
      "    teoretyk 0.44642752408981323\n",
      "    biodro 0.43907320499420166\n",
      "\n",
      "WORD: miłość\n",
      "    stłumienie 0.5130264163017273\n",
      "    rock 0.5128613114356995\n",
      "    beret 0.48549214005470276\n",
      "    lechia 0.4771725535392761\n",
      "    przeszkolenie 0.47533541917800903\n",
      "    zdyskontowanie 0.46027833223342896\n",
      "    gekon 0.45985713601112366\n",
      "    tresura 0.4564157724380493\n",
      "    rozkład 0.45604395866394043\n",
      "    rowerowy 0.44898897409439087\n",
      "\n",
      "WORD: rower\n",
      "    badawczy 0.5492117404937744\n",
      "    promil 0.5389091968536377\n",
      "    australia 0.5195481181144714\n",
      "    zapobiegliwy 0.4833507537841797\n",
      "    leśnica 0.4716686010360718\n",
      "    nik 0.45931506156921387\n",
      "    niepisany 0.45519718527793884\n",
      "    piach 0.4511232376098633\n",
      "    kastowość 0.4501056969165802\n",
      "    centrolewica 0.4480299651622772\n",
      "\n",
      "WORD: maraton\n",
      "    skierowanie 0.8447396159172058\n",
      "    wyszywanie 0.8447386026382446\n",
      "    kontynent 0.8447381854057312\n",
      "    bochen 0.8447363376617432\n",
      "    przełom 0.844730019569397\n",
      "    piana 0.8447293639183044\n",
      "    ścięcie 0.844727635383606\n",
      "    bryja 0.8447275757789612\n",
      "    obwodnica 0.8447273969650269\n",
      "    nieświadomość 0.8447273969650269\n",
      "\n",
      "WORD: logika\n",
      "    krocz 0.5058234930038452\n",
      "    zaiskrzenie 0.4845918118953705\n",
      "    ludwika 0.4845040440559387\n",
      "    trik 0.4840782880783081\n",
      "    andaluzja 0.47497373819351196\n",
      "    zasobnik 0.4666714072227478\n",
      "    grunwaldzki 0.4651784300804138\n",
      "    napełnianie 0.4606744945049286\n",
      "    pochwowy 0.45494022965431213\n",
      "    szarka 0.44776445627212524\n",
      "\n",
      "WORD: motyl\n",
      "    powinność 0.6108222603797913\n",
      "    otaczający 0.5175608992576599\n",
      "    zadaszony 0.4901060461997986\n",
      "    wino 0.48719391226768494\n",
      "    mierzwa 0.4842497408390045\n",
      "    prostota 0.47581198811531067\n",
      "    sakra 0.46237999200820923\n",
      "    nadejście 0.4484606087207794\n",
      "    kurz 0.4462658762931824\n",
      "    najpewniejszy 0.44397127628326416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
    "\n",
    "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
    "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
    "\n",
    "example_words = example_polish_words\n",
    "\n",
    "for w0 in example_words:\n",
    "    print ('WORD:', w0)\n",
    "    for w, v in task1_wv.most_similar(w0):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41228961",
   "metadata": {},
   "source": [
    "## Task 2 (4 points)\n",
    "\n",
    "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
    "\n",
    "```python\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
    "\n",
    "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
    "\n",
    "Compare these two approaches using similar code to the code from Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c1ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "# from gensim.utils import simple_preprocess\n",
    "import gensim.utils as utils\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6ad8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cell for your presentation\n",
    "wiki_links = word2vec.LineSentence(\"./data/simple.wiki.links.txt\")\n",
    "# model = Word2Vec(sentences=wiki_links, vector_size=100, window=2, min_count=1, workers=16)\n",
    "# model.save(\"wiki_v1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42622fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: adolf_hitler\n",
      "    joseph_stalin 0.9806583523750305\n",
      "    pope_john_paul_ii 0.9689143896102905\n",
      "    benito_mussolini 0.9687402248382568\n",
      "    napoleon 0.9680567383766174\n",
      "    josip_broz_tito 0.9678819179534912\n",
      "    berlin_wall 0.9678562879562378\n",
      "    nazi_party 0.9673212766647339\n",
      "    fascism 0.9652743339538574\n",
      "    paul_von_hindenburg 0.964741051197052\n",
      "    armistice 0.9642995595932007\n",
      "\n",
      "WORD: white\n",
      "    yellow 0.9943046569824219\n",
      "    ear 0.9888944029808044\n",
      "    fur 0.9888084530830383\n",
      "    plastic 0.9881877899169922\n",
      "    brown 0.9872660040855408\n",
      "    parasite 0.9870258569717407\n",
      "    rabbit 0.9870242476463318\n",
      "    ox 0.9865660667419434\n",
      "    cow 0.9858281016349792\n",
      "    ceramic 0.9856430888175964\n",
      "\n",
      "WORD: black\n",
      "    face 0.9959166049957275\n",
      "    gorilla 0.9953488707542419\n",
      "    chimpanzee 0.9950746893882751\n",
      "    cow 0.9947810769081116\n",
      "    symptom 0.994511067867279\n",
      "    force 0.9944021105766296\n",
      "    ground 0.994140088558197\n",
      "    system 0.9939526915550232\n",
      "    brass 0.9938763380050659\n",
      "    colour 0.9938370585441589\n",
      "\n",
      "WORD: dog\n",
      "    environment 0.9889212250709534\n",
      "    genetics 0.9796556234359741\n",
      "    poison 0.9793814420700073\n",
      "    alcohol 0.9759703874588013\n",
      "    anatomy 0.9742276668548584\n",
      "    rock_(geology) 0.9737467169761658\n",
      "    ice 0.9715026617050171\n",
      "    female 0.9713301062583923\n",
      "    sea 0.9710459113121033\n",
      "    nature 0.9702522158622742\n",
      "\n",
      "WORD: 67th_academy_awards\n",
      "    apocalypse_now 0.9760963320732117\n",
      "    michael_bay 0.9746452569961548\n",
      "    will_smith 0.9719870090484619\n",
      "    golden_globe_awards 0.971321165561676\n",
      "    julia_roberts 0.9704466462135315\n",
      "    miley_cyrus 0.9704241752624512\n",
      "    steve_martin 0.9700202345848083\n",
      "    star_wars_episode_iv:_a_new_hope 0.9694551825523376\n",
      "    taylor_swift 0.9693619608879089\n",
      "    cameo_appearance 0.9692366123199463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Word2Vec.load(\"wiki_v1.model\")\n",
    "\n",
    "example_words = [\"adolf_hitler\", \"white\", \"black\", \"dog\", \"67th_academy_awards\",]\n",
    "\n",
    "for w0 in example_words:\n",
    "    print ('WORD:', w0)\n",
    "    for w, v in model.wv.most_similar(w0):\n",
    "        print ('   ', w, v)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98913ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wiki_links_file = open(\"./data/simple.wiki.links.txt\", \"r\")\n",
    "\n",
    "splited_links = re.split(r\"[_ \\n(),:]\", wiki_links_file.read())\n",
    "\n",
    "splited_links = list(filter(None, splited_links))\n",
    "\n",
    "word_count = Counter(splited_links)\n",
    "\n",
    "rare_words = [word for (word, count) in word_count.items() if count <= 3 and count >= 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7004e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = set([title for link in wiki_links for title in link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "867c40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_with_word = {}\n",
    "\n",
    "for title in wiki_titles:\n",
    "    words_in_title = re.split(r\"[_ \\n(),:]\", title)\n",
    "    for word in words_in_title:\n",
    "        if word not in titles_with_word:\n",
    "            titles_with_word[word] = [title]\n",
    "        else:\n",
    "            titles_with_word[word].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb43f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_with_rare_word = { word: titles for word, titles in titles_with_word.items() if len(titles) <= 3 and len(titles) >= 2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa96e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wikt:anhydrous', 'anhydrous'], ['shah_qotb_ol_din', 'shah_qotb_ol_din_heydar'], ['feline_zoonosis', 'zoonosis'], ['file:exit_1,_shuanglian_station_20201017.jpeg', 'file:exit_2,_shuanglian_station_20210701.jpeg', 'shuanglian_metro_station'], ['thornton_cleveleys', 'cleveleys', 'blackpool_north_and_cleveleys_(uk_parliament_constituency)'], [\"lauri_ingman's_first_cabinet\", \"lauri_ingman's_second_cabinet\"], [':category:discoveries_by_wilhelm_dieckvoß', ':de:wilhelm_dieckvoß', 'wilhelm_dieckvoß'], [':wikt:equip', 'wikt:equip', ':ca:plantilla:equip_irc'], ['lobsang_tenzin', 'lobsang_sangay'], ['gpu', 'mali_(gpu)'], ['felician_of_foligno', 'primus_and_felician', 'felician_college'], ['erinn_bartlett', 'erinn_hayes'], ['grat_coalition', 'grat'], ['user:peterdownunder/cyberbullying', 'user_talk:peterdownunder/cyberbullying'], ['veyrières,_cantal', 'veyrières,_corrèze'], ['special:contributions/2804:18:1082:690b:140b:77ca:e9c0:905b', 'user_talk:2804:18:1082:690b:140b:77ca:e9c0:905b'], ['special:contributions/2804:18:1082:690b:140b:77ca:e9c0:905b', 'user_talk:2804:18:1082:690b:140b:77ca:e9c0:905b'], ['special:contributions/2804:18:1082:690b:140b:77ca:e9c0:905b', 'user_talk:2804:18:1082:690b:140b:77ca:e9c0:905b'], ['special:contributions/2804:18:1082:690b:140b:77ca:e9c0:905b', 'user_talk:2804:18:1082:690b:140b:77ca:e9c0:905b'], ['special:contributions/2804:18:1082:690b:140b:77ca:e9c0:905b', 'user_talk:2804:18:1082:690b:140b:77ca:e9c0:905b']]\n"
     ]
    }
   ],
   "source": [
    "print(list(titles_with_rare_word.values())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b37b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "related_titles = [pair for rel_titles in titles_with_rare_word.values() for pair in permutations(rel_titles) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04910274",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_links_and_related = list(wiki_links) + related_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e00874a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cell for your presentation\n",
    "model_related = Word2Vec(sentences=wiki_links_and_related, vector_size=100, window=2, min_count=1, workers=16)\n",
    "model_related.save(\"wiki_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e5bc67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: adolf_hitler\n",
      "   joseph_stalin                 0.9806583523750305            joseph_stalin                 0.9785109162330627\n",
      "   pope_john_paul_ii             0.9689143896102905            pope_john_paul_ii             0.9718990921974182\n",
      "   benito_mussolini              0.9687402248382568            benito_mussolini              0.9714155793190002\n",
      "   napoleon                      0.9680567383766174            european_commission           0.9713098406791687\n",
      "   josip_broz_tito               0.9678819179534912            irish_republican_army         0.9681921005249023\n",
      "   berlin_wall                   0.9678562879562378            new_year                      0.9675443172454834\n",
      "   nazi_party                    0.9673212766647339            weimar_republic               0.9669830799102783\n",
      "   fascism                       0.9652743339538574            louis_xiv_of_france           0.9668290615081787\n",
      "   paul_von_hindenburg           0.964741051197052             ho_chi_minh                   0.9666090607643127\n",
      "   armistice                     0.9642995595932007            socialism                     0.966508150100708\n",
      "\n",
      "WORD: white\n",
      "   yellow                        0.9943046569824219            yellow                        0.9953563809394836\n",
      "   ear                           0.9888944029808044            green                         0.9926078915596008\n",
      "   fur                           0.9888084530830383            ear                           0.9896093010902405\n",
      "   plastic                       0.9881877899169922            neutron                       0.988752543926239\n",
      "   brown                         0.9872660040855408            bone                          0.9884548783302307\n",
      "   parasite                      0.9870258569717407            black                         0.9884426593780518\n",
      "   rabbit                        0.9870242476463318            ecosystem                     0.9882407784461975\n",
      "   ox                            0.9865660667419434            white_dwarf                   0.9878784418106079\n",
      "   cow                           0.9858281016349792            parasite                      0.9874089360237122\n",
      "   ceramic                       0.9856430888175964            substance                     0.9869258403778076\n",
      "\n",
      "WORD: black\n",
      "   face                          0.9959166049957275            arm                           0.9962083101272583\n",
      "   gorilla                       0.9953488707542419            purple                        0.9958349466323853\n",
      "   chimpanzee                    0.9950746893882751            ear                           0.9957136511802673\n",
      "   cow                           0.9947810769081116            biblical_canon                0.9955511689186096\n",
      "   symptom                       0.994511067867279             nuclear_physics               0.9954881072044373\n",
      "   force                         0.9944021105766296            gorilla                       0.9954774379730225\n",
      "   ground                        0.994140088558197             papyrus                       0.9954646825790405\n",
      "   system                        0.9939526915550232            sky                           0.9954615831375122\n",
      "   brass                         0.9938763380050659            nose                          0.995429515838623\n",
      "   colour                        0.9938370585441589            behavior                      0.9953029751777649\n",
      "\n",
      "WORD: dog\n",
      "   environment                   0.9889212250709534            environment                   0.9863800406455994\n",
      "   genetics                      0.9796556234359741            anatomy                       0.9837996959686279\n",
      "   poison                        0.9793814420700073            rock_(geology)                0.9830232262611389\n",
      "   alcohol                       0.9759703874588013            sea                           0.9811888933181763\n",
      "   anatomy                       0.9742276668548584            ice                           0.981184184551239\n",
      "   rock_(geology)                0.9737467169761658            poison                        0.9808129072189331\n",
      "   ice                           0.9715026617050171            geology                       0.9798594117164612\n",
      "   female                        0.9713301062583923            nature                        0.9797893166542053\n",
      "   sea                           0.9710459113121033            genetics                      0.9781017899513245\n",
      "   nature                        0.9702522158622742            ocean                         0.9779396653175354\n",
      "\n",
      "WORD: 67th_academy_awards\n",
      "   apocalypse_now                0.9760963320732117            68th_academy_awards           0.9771059155464172\n",
      "   michael_bay                   0.9746452569961548            57th_academy_awards           0.9756726026535034\n",
      "   will_smith                    0.9719870090484619            50th_academy_awards           0.9750218987464905\n",
      "   golden_globe_awards           0.971321165561676             64th_academy_awards           0.9740430116653442\n",
      "   julia_roberts                 0.9704466462135315            the_last_emperor              0.9739129543304443\n",
      "   miley_cyrus                   0.9704241752624512            there_will_be_blood           0.9735711812973022\n",
      "   steve_martin                  0.9700202345848083            77th_academy_awards           0.9730649590492249\n",
      "   star_wars_episode_iv:_a_new_hope0.9694551825523376            42nd_academy_awards           0.9726042747497559\n",
      "   taylor_swift                  0.9693619608879089            83rd_academy_awards           0.9722416996955872\n",
      "   cameo_appearance              0.9692366123199463            35th_academy_awards           0.9721434116363525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for i in range(0, 4):\n",
    "#     print(f\"{names[i] : <10}{marks[i] : ^10}{div[i] : ^10}{id[i] : >5}\")\n",
    "\n",
    "for w0 in example_words:\n",
    "    print ('WORD:', w0)\n",
    "    for (w1, v1), (w2, v2) in zip(model.wv.most_similar(w0), model_related.wv.most_similar(w0)):\n",
    "        print(f\"   {w1 : <30}{v1 : <30}{w2 : <30}{v2 : <5}\")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5e04",
   "metadata": {},
   "source": [
    "# Task 3 (4 points)\n",
    "\n",
    "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
    "\n",
    "<pre>\n",
    "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
    "</pre>\n",
    "\n",
    "And this is its translation into Lower:\n",
    "\n",
    "<pre>\n",
    "the quick brown fox jumps over the lazy dog\n",
    "</pre>\n",
    "\n",
    "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
    "\n",
    "Prepare the corpora wich contains three kind of sentences:\n",
    "* Upper corpus sentences\n",
    "* Lower corpus sentences\n",
    "* sentences derived from Upper/Lower corpus, modified using D\n",
    "\n",
    "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
    "\n",
    "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
    "\n",
    "<pre>\n",
    "WOLF, CAT, WOLVES, LION, gopher, dog\n",
    "</pre>\n",
    "\n",
    "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab14b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_corpus(path):\n",
    "    f = open(path, \"r\")\n",
    "    return [list(utils.simple_tokenize(line)) for line in f.readlines()]\n",
    "\n",
    "def file_to_word_count(path):\n",
    "    f = open(path, \"r\")\n",
    "    tokens = list(utils.simple_tokenize(f.read()))\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0570a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_corpus = file_to_corpus(\"./data/task3_polish_lower.txt\")\n",
    "upper_corpus = file_to_corpus(\"./data/task3_polish_upper.txt\")\n",
    "lower_words_count = file_to_word_count(\"./data/task3_polish_lower.txt\")\n",
    "upper_words_count = file_to_word_count(\"./data/task3_polish_upper.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197c4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_words_sorted = sorted(lower_words_count.items(), key=lambda x: x[1], reverse=True)\n",
    "upper_words_sorted = sorted(upper_words_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "578d56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(list(lower_words_count.keys()) + list(map(lambda w: w.lower(), upper_words_count.keys())))\n",
    "all_words_count = {word: lower_words_count.get(word, 0) + upper_words_count.get(word.upper(), 0) for word in all_words}\n",
    "translated_words = sorted(all_words_count.keys(), key=lambda w: all_words_count[w], reverse=True)[:1000]\n",
    "non_translated_words = sorted(all_words_count.keys(), key=lambda w: all_words_count[w], reverse=True)[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84de1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = {w: w.upper() for w in translated_words}\n",
    "translations.update({w.upper(): w for w in translated_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c21f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'C', 'B', 'D'], ['A', 'C', 'b', 'D']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_sentence(sentence, translations):\n",
    "    return [translations[word] if word in translations.keys() else word for word in sentence]\n",
    "\n",
    "def translate_corpus(corpus, translations):\n",
    "    return [translate_sentence(sentence, translations) for sentence in corpus]\n",
    "\n",
    "def translate_sentence_2(sentence, translations):\n",
    "    if sentence == []:\n",
    "        return [[]]\n",
    "    \n",
    "    head = sentence[0]\n",
    "    tail = sentence[1:]\n",
    "\n",
    "    translated_tail = translate_sentence_2(tail, translations)\n",
    "\n",
    "    result = list(map(lambda suffix: [head] + suffix, translated_tail))\n",
    "\n",
    "    if head in translations:\n",
    "        result += list(map(lambda suffix: [translations[head]] + suffix, translated_tail))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def translate_corpus_2(corpus, translations):\n",
    "    return [ts for sentence in corpus for ts in translate_sentence_2(sentence, translations)]\n",
    "\n",
    "def translate_sentence_3(sentence, translations):\n",
    "    result = []\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word in translations:\n",
    "            result.append(sentence[:i] + [translations[word]] + sentence[i+1:])\n",
    "    return result\n",
    "\n",
    "def translate_corpus_3(corpus, translations):\n",
    "    return [ts for sentence in corpus for ts in translate_sentence_3(sentence, translations)]\n",
    "\n",
    "        \n",
    "\n",
    "test_corpus = [[\"A\"], [\"A\", \"B\"], [\"A\", \"B\", \"C\"]]\n",
    "test_translations = {\"A\": \"a\", \"B\": \"b\"}\n",
    "\n",
    "# translate_corpus_2(test_corpus, test_translations)\n",
    "translate_sentence_3([\"A\", \"C\", \"B\", \"D\"], test_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cddd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = upper_corpus + lower_corpus + translate_corpus_3(upper_corpus, translations) + translate_corpus_3(lower_corpus, translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c372543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task_3 = Word2Vec(sentences=train_corpus, vector_size=100, window=5, min_count=1, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0006880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task_3.save(\"task_3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b7628fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_score(model, word):\n",
    "    translate = lambda x: x.lower()\n",
    "    case_pred = lambda x: x[0].islower()\n",
    "\n",
    "    if word.islower():\n",
    "        translate = lambda x: x.upper()\n",
    "        case_pred = lambda x: x[0].isupper()\n",
    "    \n",
    "    try:\n",
    "        rank = model.wv.rank(word, translate(word))\n",
    "    except Exception:\n",
    "        return 0\n",
    "    most_similar = model.wv.most_similar(word, topn=rank)\n",
    "\n",
    "    return 1/len(list(filter(case_pred, most_similar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1dfa9584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores:\n",
      "Words from D: 0.99975\n",
      "Words outside D: 0.3557967452567467\n"
     ]
    }
   ],
   "source": [
    "# model_task_3 = Word2Vec.load(\"task_3.model\")\n",
    "\n",
    "print(\"Average scores:\")\n",
    "print(f\"Words from D: {np.mean([word_score(model_task_3, word) for word in translations.keys()])}\")\n",
    "words_to_test = np.random.choice(non_translated_words, size=2000, replace=False)\n",
    "print(f\"Words outside D: {np.mean([word_score(model_task_3, word) for word in words_to_test])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947e307",
   "metadata": {},
   "source": [
    "# Task 4 (4 points)\n",
    "\n",
    "In this task you are asked to do two things:\n",
    "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
    "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
    "\n",
    "You can use the following code snippets:\n",
    "\n",
    "```python\n",
    "# printing tokenized Brown Corpora\n",
    "from nltk.corpus import brown\n",
    "for s in brown.sents():\n",
    "    print(*s)\n",
    "    \n",
    "#iterating over all synsets in WordNet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
    "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
    "        print (synset.definition())\n",
    "        print (synset.examples())\n",
    "        print ([lem.name() for lem in synset.lemmas()])\n",
    "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
    "        \n",
    "# loading model and compute cosine similarity between words\n",
    "\n",
    "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
    "print (model.wv.similarity('dog', 'cat'))\n",
    "```\n",
    "\n",
    "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
    "1. Tokenized Brown Corpora\n",
    "2. Definitions and examples from Princeton WordNet\n",
    "3. (1) and (2) together\n",
    "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
    "\n",
    "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for computing correlation between W2V similarity, and human judgements\n",
    "\n",
    "import gensim.downloader\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "gn = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "for similarity_type in ['relatedness', 'similarity']:\n",
    "    ws353 = []\n",
    "    for x in open(f'wordsim_{similarity_type}_goldstandard.txt'): \n",
    "        a,b,val = x.split()\n",
    "        val = float(val)\n",
    "        ws353.append( (a,b,val))\n",
    "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
    "    print (similarity_type + ':', spearmanr(vals, ys)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
